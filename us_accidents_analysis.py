# -*- coding: utf-8 -*-
"""us-accidents-analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vTe96cxrc6SJBmVWKry9EyW-p09BBIwR

# **Exploratory Data Analysis on US Accidents dataset**

# US Accidents Exploratory Data Analysis

Exploratory Data Analysis (EDA) is an approach to analyze the data using visual techniques. It is used to discover trends, patterns, or to check assumptions with the help of statistical summary and graphical representations. 

The dataset used here is a countrywide accident dataset, with data from Feb 2016 to the end of Dec 2021. This dataset has been collected in real-time, using multiple Traffic APIs. I have downloaded this dataset from Kaggle.

Lets download the dataset from Kaggle
"""

pip install opendatasets --upgrade --quiet

import opendatasets as od

download_url = 'https://www.kaggle.com/sobhanmoosavi/us-accidents'

od.download(download_url)

data_filename = '/content/us-accidents/US_Accidents_Dec21_updated.csv'

"""## Data Preparation and Cleaning

1. Load the file using Pandas
2. Look at some information about the data & the columns
3. Fix any missing or incorrect values
"""

import pandas as pd

df = pd.read_csv(data_filename)

df

"""There are 2845342 rows and 47 columns.
Lets get some information about data type of columns.
"""

df.info()

"""Lets get some statistical summary."""

df.describe()

"""Lets check number of numeric columns."""

numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']

numeric_df = df.select_dtypes(include=numerics)
len(numeric_df.columns)

"""Lets calculate percentage of missing values per column."""

missing_percentages = df.isna().sum().sort_values(ascending=False) / len(df)
missing_percentages

# missing_percentages[missing_percentages!=0] keeps the rows which have non zero values.

type(missing_percentages)

"""Lets view this as a graph. Here we prefer Bar graph over line graph because line graphs are useful for displaying smaller changes in a trend over time. Bar graphs are better for comparing larger changes or differences in data among groups."""

missing_percentages[missing_percentages != 0].plot(kind='barh')

"""We can remove columns that you don't want to use by dropping them.

## Exploratory Analysis and Visualization
As its not possible to analyze all columns at the same time, you need to analyze those columns which you think might give fruitful insights.
Lets pick numeric columns that are related to accidents.
Lets analyze the following columns:

1. City
2. Start Time
3. Start Lat, Start Lng
"""

df.columns

"""### City"""

df.City

cities = df.City.unique()
len(cities)

"""There are 11,682 unique cities. Lets calculate number of accidents per city."""

cities_by_accident = df.City.value_counts()
cities_by_accident

"""Lets analyze top 20 cities with highest number of accidents. It might be possible that there are less accidents occurring in some city because of less population over there. If there was population column in the dataset, we could have calculated ----------per capita."""

cities_by_accident[:20]

type(cities_by_accident)

cities_by_accident[:20].plot(kind='barh')

"""
Lets look at the distribution whether lot of cities have small number of accidents or  high number of accidents. 
To get such a distribution, plotting Histogram is the best."""

import seaborn as sns
sns.set_style("darkgrid")

sns.distplot(cities_by_accident)

high_accident_cities = cities_by_accident[ cities_by_accident >= 1000]
low_accident_cities = cities_by_accident[ cities_by_accident < 1000]

len(high_accident_cities) / len(cities)

"""Less than 5% of cities have more than 1000 yearly accidents."""

sns.distplot(high_accident_cities)

sns.distplot(low_accident_cities)

"""Both are exponentially decreasing kind of graphs. Seems like number of accidents per city seems to follow some sort of exponential distribution. So we can use a Logarithmic scale."""

sns.histplot(cities_by_accident, log_scale=True)

"""Maybe some part of the data is missing so we got here that lot of cities have 0 accidents and that's wrong. Around 1100 cities have no data or maybe they are cities with number of accidents as 1.
Between 0 to 100 is where major chunk of the data falls.
"""

cities_by_accident[cities_by_accident == 1]

"""Over 1000 cities have reported just one accident.

### Start Time
"""

df.Start_Time

# Lets change the string to date
df.Start_Time = pd.to_datetime(df.Start_Time)

# norm hist gives percentage and not count
sns.distplot(df.Start_Time.dt.hour, bins=24, kde=False, norm_hist=True)

"""- A high percentage of accidents occur between 2 pm to 7 pm.

Let's find out which day of the week had more number of accidents has occurred.
"""

sns.distplot(df.Start_Time.dt.dayofweek, bins=7, kde=False, norm_hist=True)

"""Number of accidents is lower on weekends compared to week days."""

sundays_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 6]
sns.distplot(sundays_start_time.dt.hour, bins=24, kde=False, norm_hist=True)

"""Most accidents occured during the afternoon time on Sundays"""

monday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 0]
sns.distplot(monday_start_time.dt.hour, bins=24, kde=False, norm_hist=True)

"""Lets look at the month wise trend."""

sns.distplot(df.Start_Time.dt.month, bins=12, kde=False, norm_hist=True)

"""More number of accidents took place in the last months of the year.

Month-wise trend in a particular year:
"""

df_2018 = df[df.Start_Time.dt.year == 2018]
sns.distplot(df_2018.Start_Time.dt.month, bins=12, kde=False, norm_hist=True)

"""### Start Latitude & Longitude

"""

df.Start_Lat

df.Start_Lng

# as the data is large, let's create a 10% sample
sample_df = df.sample(int(0.1 * len(df)))

sns.scatterplot(x=sample_df.Start_Lng, y=sample_df.Start_Lat, size=0.001)

"""Let's represent this in a world Map."""

import folium

#lat, lon = df.Start_Lat[0], df.Start_Lng[0]
#lat, lon

#for x in df[['Start_Lat', 'Start_Lng']].sample(100).iteritems():
    #print(x[1])

# list(df.Start_Lat), list(df.Start_Lng) means list of latitudes and list pf longitudes.
zip(list(df.Start_Lat), list(df.Start_Lng))

from folium.plugins import HeatMap

sample_df = df.sample(int(0.001 * len(df)))
lat_lon_pairs = list(zip(list(sample_df.Start_Lat), list(sample_df.Start_Lng)))

map = folium.Map()
HeatMap(lat_lon_pairs).add_to(map)
map

"""## Summary and Conclusion


Insights:
- No data from New York
- The number of accidents per city decreases exponentially
- Less than 5% of cities have more than 1000 yearly accidents.
- Over 1000 cities have reported just one accident.

"""